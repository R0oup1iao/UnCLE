import os
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

class CausalTimeSeriesDataset(Dataset):
    """
    ä¸“ä¸šçš„æ—¶åº Datasetï¼Œæ”¯æŒæ»‘åŠ¨çª—å£åˆ‡ç‰‡ï¼Œä½å†…å­˜å ç”¨ã€‚
    """
    def __init__(self, data, window_size, stride=1, mode='train', split_ratio=0.8):
        super().__init__()
        self.window_size = window_size
        self.stride = stride
        
        # 1. æ•°æ®åˆ‡åˆ† (æŒ‰æ—¶é—´è½´åˆ‡åˆ†è®­ç»ƒ/éªŒè¯é›†)
        split_point = int(len(data) * split_ratio)
        if mode == 'train':
            self.data = data[:split_point]
        elif mode == 'val':
            self.data = data[split_point:]
        else:
            raise ValueError(f"Unknown mode: {mode}")

        # 2. è®¡ç®—æ ·æœ¬æ€»é‡
        if len(self.data) < window_size:
            self.n_samples = 0
        else:
            self.n_samples = (len(self.data) - window_size) // stride + 1

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        start = idx * self.stride
        end = start + self.window_size
        sample = self.data[start:end]
        # (T, N) -> (N, T)
        sample_tensor = torch.from_numpy(sample).float().t() 
        return sample_tensor

def load_from_disk(base_path, dataset_name, replica_id):
    """
    è¯»å–ç£ç›˜æ–‡ä»¶ (Numpyæ ¼å¼)ã€‚
    [ä¿®æ”¹] å¢åŠ å¯¹çœŸå®æ•°æ®çš„å…¼å®¹æ€§ï¼šGT å’Œ Coords å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è¿”å›é»˜è®¤å€¼ã€‚
    """
    data_dir = os.path.join(base_path, dataset_name)
    data_path = os.path.join(data_dir, f'data_{replica_id}.npy')
    gt_path = os.path.join(data_dir, f'gt_{replica_id}.npy')
    coords_path = os.path.join(data_dir, f'coords_{replica_id}.npy')

    # 1. å¿…é¡»è¦æœ‰ Data
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ Data not found: {data_path}")
    data_np = np.load(data_path) # Shape: (T, N)
    N = data_np.shape[1]

    # 2. GT æ˜¯å¯é€‰çš„ (çœŸå®æ•°æ®é€šå¸¸æ²¡æœ‰)
    if os.path.exists(gt_path):
        gt_np = np.load(gt_path) # Shape: (N, N)
    else:
        print(f"âš ï¸ Warning: Ground Truth not found at {gt_path}. Metrics will be skipped.")
        gt_np = None

    # 3. Coords ä¹Ÿæ˜¯å¯é€‰çš„ (å¦‚æœæ²¡æœ‰ï¼Œéšæœºç”Ÿæˆä»¥é€‚é… ST_CausalFormer)
    if os.path.exists(coords_path):
        coords_np = np.load(coords_path) # Shape: (N, 2)
    else:
        print(f"âš ï¸ Warning: Coords not found at {coords_path}. Using random coordinates for spatial clustering.")
        # ç”Ÿæˆéšæœºåæ ‡ (N, 2)
        np.random.seed(42)
        coords_np = np.random.rand(N, 2)
    
    return data_np, gt_np, coords_np

def get_data_context(args):
    """
    å·¥å‚å‡½æ•°ï¼šè¿”å› Train/Val Loaders å’Œ Meta
    """
    base_path = getattr(args, 'data_path', 'data/synthetic')
    dataset_name = getattr(args, 'dataset', 'lorenz96')
    replica_id = getattr(args, 'replica_id', 0)
    
    window_size = getattr(args, 'window_size', 100) 
    stride = getattr(args, 'stride', 10)
    batch_size = getattr(args, 'batch_size', 32)

    print(f"ğŸ“‚ Loading {dataset_name} (Replica {replica_id})...")
    
    # åŠ è½½æ•°æ® (å…¼å®¹æ¨¡å¼)
    data_np, gt_np, coords_np = load_from_disk(base_path, dataset_name, replica_id)

    # 1. æ—¶åºæ•°æ®æ ‡å‡†åŒ– (Z-Score)
    # è¿™ä¸€æ­¥å¯¹ Transformer è®­ç»ƒç¨³å®šè‡³å…³é‡è¦
    mean = data_np.mean(axis=0)
    std = data_np.std(axis=0) + 1e-5
    data_np = (data_np - mean) / std

    # 2. [å…³é”®ä¿®å¤] åæ ‡æ•°æ®å½’ä¸€åŒ– (Min-Max -> [-1, 1])
    # é˜²æ­¢åæ ‡æ•°å€¼è¿‡å¤§ï¼ˆå¦‚ç»çº¬åº¦æˆ–ç±³åˆ¶åæ ‡ï¼‰ä¸»å¯¼ LearnableSpatialPooler çš„çº¿æ€§å±‚ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–æ¨¡å¼åå¡Œã€‚
    c_min = coords_np.min(axis=0)
    c_max = coords_np.max(axis=0)
    denom = c_max - c_min
    denom[denom == 0] = 1.0 # é˜²æ­¢é™¤ä»¥0
    
    coords_np = 2 * (coords_np - c_min) / denom - 1.0
    print(f"ğŸ“ Coords Normalized to [-1, 1]. Original Shape: {coords_np.shape}")

    train_ds = CausalTimeSeriesDataset(data_np, window_size, stride, mode='train')
    val_ds = CausalTimeSeriesDataset(data_np, window_size, stride, mode='val')

    print(f"âœ… Data Split: Train={len(train_ds)} samples, Val={len(val_ds)} samples")
    
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

    meta = {
        "coords": coords_np,
        "gt_fine": gt_np,   # å¯èƒ½æ˜¯ None
        "gt_coarse": None, 
        "patch_ids": None
    }
    
    return train_loader, val_loader, meta