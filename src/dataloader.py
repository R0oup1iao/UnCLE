import os
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

class CausalTimeSeriesDataset(Dataset):
    """
    ä¸“ä¸šçš„æ—¶åº Datasetï¼Œæ”¯æŒæ»‘åŠ¨çª—å£åˆ‡ç‰‡ï¼Œä½å†…å­˜å ç”¨ã€‚
    """
    def __init__(self, data, window_size, stride=1, mode='train', split_ratio=0.8):
        """
        args:
            data: np.ndarray, shape (T_total, N)
            window_size: int, æ—¶é—´çª—å£é•¿åº¦ (T)
            stride: int, æ»‘åŠ¨æ­¥é•¿
            mode: 'train' or 'val'
            split_ratio: è®­ç»ƒé›†å æ¯”
        """
        super().__init__()
        self.window_size = window_size
        self.stride = stride
        
        # 1. æ•°æ®åˆ‡åˆ† (æŒ‰æ—¶é—´è½´åˆ‡åˆ†è®­ç»ƒ/éªŒè¯é›†)
        split_point = int(len(data) * split_ratio)
        if mode == 'train':
            self.data = data[:split_point]
        elif mode == 'val':
            self.data = data[split_point:]
        else:
            raise ValueError(f"Unknown mode: {mode}")

        # 2. è®¡ç®—æ ·æœ¬æ€»é‡
        # å…¬å¼: (Total_Len - Window_Len) // Stride + 1
        if len(self.data) < window_size:
            self.n_samples = 0
        else:
            self.n_samples = (len(self.data) - window_size) // stride + 1

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        """
        æ ¸å¿ƒï¼šåªåœ¨éœ€è¦æ—¶åˆ‡ç‰‡ï¼Œä¸å ç”¨é¢å¤–å†…å­˜
        """
        # è®¡ç®—çœŸå®çš„åˆ‡ç‰‡ç´¢å¼•
        start = idx * self.stride
        end = start + self.window_size
        
        # åˆ‡ç‰‡: (T_window, N)
        sample = self.data[start:end]
        
        # è½¬æ¢: Numpy -> Tensor
        # å½¢çŠ¶å˜æ¢: (T, N) -> (N, T) ä»¥é€‚é…ä½ çš„ ST_CausalFormer è¾“å…¥ (Batch, N, T)
        sample_tensor = torch.from_numpy(sample).float().t() 
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ return targetï¼Œå› ä¸ºå› æœå‘ç°é€šå¸¸æ˜¯è‡ªç›‘ç£çš„ (reconstruction)
        # å¦‚æœä½ éœ€è¦ target (æ¯”å¦‚ next step prediction)ï¼Œå¯ä»¥åœ¨è¿™é‡Œç”± sample åˆ‡åˆ†å‡ºæ¥
        return sample_tensor

def load_from_disk(base_path, dataset_name, replica_id):
    """è¯»å–ç£ç›˜æ–‡ä»¶ (Numpyæ ¼å¼)"""
    data_dir = os.path.join(base_path, dataset_name)
    data_path = os.path.join(data_dir, f'data_{replica_id}.npy')
    gt_path = os.path.join(data_dir, f'gt_{replica_id}.npy')
    coords_path = os.path.join(data_dir, f'coords_{replica_id}.npy')

    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ Data not found: {data_path}")

    data_np = np.load(data_path)     # Shape: (T, N)
    gt_np = np.load(gt_path)         # Shape: (N, N)
    coords_np = np.load(coords_path) # Shape: (N, 2)
    
    return data_np, gt_np, coords_np

def get_data_context(args):
    """
    å·¥å‚å‡½æ•°ï¼šè¿”å› Train/Val Loaders å’Œ Meta
    """
    base_path = getattr(args, 'data_path', 'data/synthetic')
    dataset_name = getattr(args, 'dataset', 'lorenz96')
    replica_id = getattr(args, 'replica_id', 0)
    
    # 1. å‚æ•°é…ç½®
    # çœŸå®æ•°æ®å¾€å¾€å¾ˆé•¿ï¼Œæˆ‘ä»¬ä¸èƒ½æŠŠæ•´ä¸ª T=10000 å¡è¿›æ¨¡å‹
    # æˆ‘ä»¬åˆ‡æˆå°çª—å£ï¼Œæ¯”å¦‚ T_window=100
    window_size = getattr(args, 'window_size', 100) 
    stride = getattr(args, 'stride', 10) # æ­¥é•¿ï¼Œè¶Šå°æ•°æ®è¶Šå¤š
    batch_size = getattr(args, 'batch_size', 32)

    print(f"ğŸ“‚ Loading {dataset_name} (Replica {replica_id})...")
    
    # 2. åŠ è½½åŸå§‹å¤§çŸ©é˜µ
    data_np, gt_np, coords_np = load_from_disk(base_path, dataset_name, replica_id)

    # 3. æ ‡å‡†åŒ– (Z-Score)
    mean = data_np.mean(axis=0)
    std = data_np.std(axis=0) + 1e-5
    data_np = (data_np - mean) / std

    # 4. å®ä¾‹åŒ– Dataset (Train / Val)
    train_ds = CausalTimeSeriesDataset(data_np, window_size, stride, mode='train')
    val_ds = CausalTimeSeriesDataset(data_np, window_size, stride, mode='val')

    print(f"âœ… Data Split: Train={len(train_ds)} samples, Val={len(val_ds)} samples")
    print(f"   Window Size: {window_size}, Stride: {stride}")

    # 5. æ„é€  Loader
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

    # 6. Meta ä¿¡æ¯
    meta = {
        "coords": coords_np,
        "gt_fine": gt_np,
        "gt_coarse": None, 
        "patch_ids": None
    }
    
    return train_loader, val_loader, meta